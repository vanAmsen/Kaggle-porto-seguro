{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to do some data preparation for the models. We will create some scripts to be used in the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#import joypy\n",
    "import re\n",
    "#from IPython.display import display, HTML\n",
    "#import ipywidgets as widgets # for later\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test  = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My own function - It is less efficient\n",
    "\n",
    "def Create_OHE(dataframe,variables,limit):\n",
    "    df = dataframe.copy()\n",
    "    for variable in variables:\n",
    "        if len(pd.unique(df[variable])) < limit and len(pd.unique(df[variable])) > 2:\n",
    "            columnsv = [variable + '_ohe' + str(value) for value in pd.unique(df[variable]).tolist()]\n",
    "            dummy = pd.get_dummies(df[variable])\n",
    "            dummy.columns = columnsv\n",
    "            df = df.drop(variable,axis=1).copy().join(dummy)\n",
    "            #print(variable,columnsv)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MetaDataTypes(df,types_var=['cat','bin','target']):\n",
    "    # Classifying the variables in the data\n",
    "    variables = []\n",
    "    vartype = {}\n",
    "    for variable in df.columns:\n",
    "        for types in types_var:\n",
    "            ty = \"None\"\n",
    "            if df[variable].dtype == int:\n",
    "                tybin = \"ordinal\"\n",
    "            elif df[variable].dtype == float:\n",
    "                tybin = \"continuous\"\n",
    "            match = re.search('^.*'+types+'.*$',variable)\n",
    "            if match:\n",
    "                ty = types\n",
    "                if re.search('^.*bin.*$',variable):\n",
    "                    tybin='binary'\n",
    "                if re.search('^.*cat.*$',variable):\n",
    "                    tybin='categorical'\n",
    "                if 'target' in variable:\n",
    "                    tybin = 'binary'\n",
    "                break\n",
    "\n",
    "            \n",
    "        variables.append([variable,ty,tybin])\n",
    "        \n",
    "    variablesdf = pd.DataFrame(variables,columns=['name','type','bin'])\n",
    "    \n",
    "    for i in ['ordinal','continuous','binary','categorical']:\n",
    "        vartype[i]=variablesdf.name[(variablesdf.bin==i)]\n",
    "\n",
    "    # Creating dataframe containing variables\n",
    "    return variablesdf,vartype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types_var = ['ind','reg','car', 'calc','target','id']\n",
    "variablesdf,vartype = MetaDataTypes(train,types_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating high cardinality categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using bayesian estimator for highcardinality categorical data ('https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MICC_CARDINALITY_TRANSFORM(dataframe,target, variables, k=1, f=1,heuristic = False, drop=True):\n",
    "    \n",
    "    def lb_card(n,k,f):\n",
    "        lb = 1/(1+np.exp(-(n-k)/f))\n",
    "        return lb\n",
    "    if type(variables) == str:\n",
    "        variables = [variables]\n",
    "    for variable in variables:\n",
    "        \n",
    "        df = dataframe.copy()\n",
    "        g = pd.DataFrame(df.groupby(variable).count().iloc[:,0]).reset_index()\n",
    "        p = pd.DataFrame(df.groupby(variable).count().iloc[:,0]).reset_index()\n",
    "\n",
    "        if heuristic == True:\n",
    "            k = df[target][df[target]>0].count()/2\n",
    "            f = df[target].count()/10000*5\n",
    "        posterior = pd.DataFrame(df[df[target]>0]\n",
    "                                 .groupby(variable).count().iloc[:,0]).reset_index().iloc[:,-1]/g.iloc[:,-1]\n",
    "        prior = df[df[target]>0].count().iloc[0]/df.count().iloc[0]\n",
    "\n",
    "        variable_tf = lb_card(g.iloc[:,-1],k,f)*posterior + (1-lb_card(g.iloc[:,-1],k,f))*prior\n",
    "\n",
    "        g.iloc[:,-1] = variable_tf.values\n",
    "        g['prior'] = prior\n",
    "        g['posterior'] = posterior\n",
    "\n",
    "        df[variable+'_micc'] = df.merge(g,on=variable,how='left').iloc[:,-1]\n",
    "        \n",
    "        if drop==True:\n",
    "            df.drop(variable,axis=1,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FREQUENCY_CARDINALITY(dataframe,variables,drop=True):\n",
    "    \n",
    "    if type(variables) == str:\n",
    "        variables = [variables]\n",
    "        \n",
    "    for variable in variables:\n",
    "        df = dataframe.copy()\n",
    "        g = pd.DataFrame(df.groupby(variable).count().iloc[:,0]).reset_index()\n",
    "        g.iloc[:,-1] = g.iloc[:,-1]/df.count().iloc[0]\n",
    "        g.iloc[:,-1] = g.sort_values(by=g.columns[-1]).cumsum().iloc[:,-1]\n",
    "        df[variable+'_freq'] = df.merge(g,on=variable,how='left').iloc[:,-1]\n",
    "        \n",
    "        if drop==True:\n",
    "            df.drop(variable,axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_prep = train.copy()\n",
    "test_prep  = test.copy()\n",
    "train_prep = Create_OHE(train_prep,vartype['categorical'].values,19)\n",
    "test_prep = Create_OHE(test_prep,vartype['categorical'].values,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CAT_TRANSFORM(dataframe,dataframe_prep,variables, limitinf=20,limitsup=200):\n",
    "# limitinf = 20 # the number to consider high cardinality. Variables below will be set with one-hot encoding\n",
    "# limitsup = 50 # the number to consider very-high cardinality. Variables above will be set with one-hot encoding\n",
    "    df = dataframe.copy()\n",
    "    df_prep = dataframe_prep.copy()\n",
    "\n",
    "    for variable in variables:\n",
    "\n",
    "        if len(pd.unique(df[variable])) < limitinf and len(pd.unique(df[variable])) > 2:\n",
    "            pass\n",
    "\n",
    "        elif len(pd.unique(df[variable])) < limitsup and len(pd.unique(df[variable])) >= limitinf:\n",
    "            df_prep = FREQUENCY_CARDINALITY(df_prep,variable)\n",
    "            print('freq',variable)\n",
    "\n",
    "        elif len(pd.unique(df[variable])) >= limitsup:\n",
    "            df_prep = MICC_CARDINALITY_TRANSFORM(df_prep,'target',variable,heuristic=True)\n",
    "            print('micci',variable)\n",
    "    \n",
    "    return df_prep\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq ps_car_11_cat\n"
     ]
    }
   ],
   "source": [
    "train_prep = CAT_TRANSFORM(train, train_prep,vartype['categorical']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prep['ps_car_11_cat'] = train['ps_car_11_cat']\n",
    "key_cat = train_prep[['ps_car_11_cat','ps_car_11_cat_freq']]\n",
    "key_cat = key_cat[key_cat[['ps_car_11_cat','ps_car_11_cat_freq']].duplicated()==False]\n",
    "train_prep.drop(['ps_car_11_cat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep = test_prep.merge(right=key_cat,on=['ps_car_11_cat']).drop('ps_car_11_cat',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prep = test_prep[train_prep.drop(['target'],axis=1).columns.values] # Adjust the columns order\n",
    "train_prep.drop(['target'],axis=1).columns == test_prep.columns #Check to see if all are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davi/Envs/machinepy3/machinepy3/lib/python3.5/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'FitnessMulti' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "/home/davi/Envs/machinepy3/machinepy3/lib/python3.5/site-packages/deap/creator.py:141: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  RuntimeWarning)\n",
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "79.11827201666667 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: LogisticRegression(input_matrix, C=25.0, dual=False, penalty=l1)\n",
      "0.627500486634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using tiny fraction of data to test tpot\n",
    "# X_train, X_test, y_train, y_test = train_test_split(np.array(train_prep.drop(['target'],axis=1).iloc[0:10000,:]),\n",
    "#                                                     np.array(train_prep.target[0:10000]),\n",
    "#                                                     train_size=0.75, test_size=0.25)\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(train_prep.drop(['target'],axis=1)),\n",
    "                                                    np.array(train_prep.target),\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=20,config_dict='TPOT light',\n",
    "                      verbosity=2,scoring= 'roc_auc',n_jobs=-1,random_state=42,max_time_mins=60,cv=3)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_pipeline_porto.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Score on the training set was:0.6300687794243328\n",
    "exported_pipeline = LogisticRegression(C=25., dual=False, penalty=\"l1\")\n",
    "# X = np.array(train_prep.drop(['target'],axis=1))\n",
    "X = np.array(train_prep.drop(['target'],axis=1))\n",
    "y = np.array(train_prep.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cvscore = cross_val_score(exported_pipeline,X,y,scoring='roc_auc',cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62819730029799248"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvscore.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exported_pipeline.fit(X,y)\n",
    "id_test = test_prep['id'].values\n",
    "y_pred = exported_pipeline.predict_proba(test_prep)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_pred\n",
    "sub.to_csv('submission1_logistictpot1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892816"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(id_test == test_prep.id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (machinepy3)",
   "language": "python",
   "name": "machinepy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
